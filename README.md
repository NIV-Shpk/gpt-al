# gpt-al
A repository that shows how to finetune a GPT-J model on 8 Bit precision on an Albanian dataset. Adapted from [here](https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es?usp=sharing).

This could be adapted also for larger GPT models. It is ideal when having GPU memory limitations such as single GPU.

It will require still at least 16GB of GPU.

